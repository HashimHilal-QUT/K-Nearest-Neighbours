{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b8675d-9603-46a6-91c9-4a0a1b6fa1f3",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours â€“ Challenge: Adult Income Classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is designed as a hands-on coding challenge for beginners. You'll implement your own k-NN classifier to predict whether someone's annual income exceeds $50K.\n",
    "\n",
    "**Goal Question:** Can we predict whether someone's annual income exceeds $50K based on their personal and work attributes?\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "**Data Source:** [Income Dataset from Kaggle](https://www.kaggle.com/datasets/mastmustu/income)\n",
    "\n",
    "### Dataset Details:\n",
    "- **Total samples:** 32,561 adults\n",
    "- **Features:** 14 attributes (mix of numerical and categorical)\n",
    "- **Target:** Income bracket (â‰¤50K or >50K)\n",
    "- **Real-world application:** Economic analysis, policy making, demographic studies\n",
    "\n",
    "### Features in Our Dataset:\n",
    "\n",
    "| Feature | Description | Type | Example Values |\n",
    "|---------|-------------|------|----------------|\n",
    "| **age** | Age in years | Numerical | 25, 38, 44 |\n",
    "| **workclass** | Employment type | Categorical | Private, Self-emp, Gov |\n",
    "| **fnlwgt** | Census sampling weight | Numerical | 226802, 89814 |\n",
    "| **education** | Education level | Categorical | HS-grad, Bachelors, Masters |\n",
    "| **educational-num** | Education years | Numerical | 9, 13, 16 |\n",
    "| **marital-status** | Marital status | Categorical | Married, Single, Divorced |\n",
    "| **occupation** | Job type | Categorical | Tech-support, Craft-repair |\n",
    "| **relationship** | Family relationship | Categorical | Husband, Wife, Own-child |\n",
    "| **race** | Race | Categorical | White, Black, Asian-Pac-Islander |\n",
    "| **gender** | Gender | Categorical | Male, Female |\n",
    "| **capital-gain** | Capital gains | Numerical | 0, 7688, 3103 |\n",
    "| **capital-loss** | Capital losses | Numerical | 0, 1848, 323 |\n",
    "| **hours-per-week** | Work hours per week | Numerical | 40, 50, 30 |\n",
    "| **native-country** | Country of origin | Categorical | United-States, Mexico, India |\n",
    "\n",
    "**Target Variable:** `income` - Income bracket (â‰¤50K or >50K)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Through this exercise, you will:\n",
    "- Handle missing values in real-world data\n",
    "- Encode categorical variables for machine learning\n",
    "- Apply feature scaling for k-NN\n",
    "- Implement k-NN classification with hyperparameter tuning\n",
    "- Evaluate binary classification with confusion matrix\n",
    "- Understand precision, recall, and class imbalance\n",
    "\n",
    "## Instructions\n",
    "\n",
    "**Reference Material:** Look at `Glass Classification.ipynb` for coding examples and patterns to follow.\n",
    "\n",
    "ðŸ’¡ **Key Reminders:**\n",
    "- This is a **binary classification** problem (2 classes: â‰¤50K, >50K)\n",
    "- k-NN requires **feature scaling** - very important!\n",
    "- Use **confusion matrix** as your primary evaluation metric\n",
    "- Handle **missing values** (marked as '?' in the dataset)\n",
    "- **Encode categorical variables** before using k-NN\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9603a80-d384-4db9-ab17-46f905babb6d",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "You'll need several Python libraries for this exercise. Import the essential tools for:\n",
    "- **Data manipulation:** pandas, numpy\n",
    "- **Visualisation:** matplotlib, seaborn  \n",
    "- **Machine learning:** scikit-learn modules\n",
    "- **Data preprocessing:** StandardScaler, LabelEncoder, train_test_split\n",
    "- **k-NN algorithm:** KNeighborsClassifier\n",
    "- **Evaluation:** classification_report, confusion_matrix, cross_val_score\n",
    "\n",
    "ðŸ’¡ **Hint:** Look at the Glass Classification notebook to see which specific imports you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107048c5-e9ea-40f8-87b7-637cc3d59012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all required libraries here\n",
    "# Refer to Glass Classification.ipynb for the exact imports needed\n",
    "\n",
    "# Data manipulation\n",
    "\n",
    "\n",
    "# Visualisation  \n",
    "\n",
    "\n",
    "# Machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e65e1ff-63a5-44dd-9f20-ea85932c551d",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "\n",
    "Your first task is to load the Adult dataset and understand its structure. The dataset is stored in `Data/adult.csv`.\n",
    "\n",
    "### What to do:\n",
    "1. **Load the data** using pandas\n",
    "2. **Examine the shape** - How many samples and features?\n",
    "3. **Check for missing values** - Look for '?' entries\n",
    "4. **Display basic statistics** for numerical columns\n",
    "5. **Show value counts** for categorical columns\n",
    "6. **Check the target distribution** - How balanced are the income classes?\n",
    "\n",
    "### Key Questions to Answer:\n",
    "- How many people earn â‰¤50K vs >50K?\n",
    "- Which features have missing values?\n",
    "- What are the most common values for categorical features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313a445-c6e3-4452-b892-3144f616fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the adult.csv dataset\n",
    "\n",
    "\n",
    "# TODO: Explore the dataset structure\n",
    "\n",
    "\n",
    "# TODO: Check for missing values (look for '?' entries)\n",
    "\n",
    "\n",
    "# TODO: Check target variable distribution\n",
    "\n",
    "\n",
    "# TODO: Display basic info about the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedcb18-a886-46d0-a773-253a7ea601dd",
   "metadata": {},
   "source": [
    "## Step 3: Visualise the Data\n",
    "\n",
    "Create visualisations to understand patterns in the data. This will help you understand which features might be important for predicting income.\n",
    "\n",
    "### Suggested Visualisations:\n",
    "1. **Income distribution** - Bar chart of â‰¤50K vs >50K counts\n",
    "2. **Age vs Income** - Box plot or histogram showing age distribution by income bracket\n",
    "3. **Education vs Income** - Count plot showing income by education level\n",
    "4. **Work hours vs Income** - Box plot of hours-per-week by income bracket\n",
    "5. **Correlation heatmap** - For numerical features only\n",
    "\n",
    "### What to Look For:\n",
    "- Which age groups tend to earn more?\n",
    "- How does education level relate to income?\n",
    "- Are there clear patterns that k-NN could learn from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373b31b-1afa-4bbb-b38a-1d93b8a6bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualisations to understand the data\n",
    "# Set up plotting style similar to Glass Classification notebook\n",
    "\n",
    "# TODO: 1. Income distribution bar chart\n",
    "# TODO: 2. Age vs Income box plot\n",
    "# TODO: 3. Education vs Income (you'll need to handle this after data cleaning)\n",
    "# TODO: 4. Hours per week vs Income\n",
    "# TODO: 5. Correlation heatmap for numerical columns only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e455fd-bf1c-4984-b9db-463992ebc441",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing - The Most Important Step\n",
    "\n",
    "This is where the real work happens! You need to clean and prepare the data for k-NN classification.\n",
    "\n",
    "### Major Preprocessing Tasks:\n",
    "\n",
    "1. **Handle Missing Values:** Replace '?' with the most common value or remove rows\n",
    "2. **Encode Categorical Variables:** Convert text categories to numbers using LabelEncoder or One-Hot Encoding\n",
    "3. **Feature Scaling:** Standardise all features (Crucial for k-NN)\n",
    "4. **Train/Test Split:** Split data for proper evaluation\n",
    "\n",
    "### Why Each Step Matters:\n",
    "\n",
    "- **Missing values:** k-NN can't handle missing data\n",
    "- **Categorical encoding:** k-NN needs numerical input\n",
    "- **Feature scaling:** Different scales (age vs capital-gain) would bias distance calculations  \n",
    "- **Train/test split:** Need unseen data to evaluate model fairly\n",
    "\n",
    "### Key Challenge:\n",
    "This dataset has many categorical features, unlike the Glass Classification dataset. You'll need to decide:\n",
    "- Which categorical features to keep?\n",
    "- How to encode them (LabelEncoder vs One-Hot)?\n",
    "- How to handle the many categories in some features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ce2da-b55e-438e-9197-a3fce0a4ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Data Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac550fc-12da-42a4-99fc-42a05a906e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyperparameter Tuning for k-NN\n",
    "# Find the best k value using cross-validation (similar to Glass Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5fb73-5394-4515-99a3-e2229ffe48db",
   "metadata": {},
   "source": [
    "## Step 5: Train Final Model and Evaluate Performance\n",
    "\n",
    "Now train your final k-NN model using the best k value and evaluate it on the test set.\n",
    "\n",
    "### Evaluation Focus:\n",
    "Since this is a **binary classification** problem, pay special attention to:\n",
    "- **Overall accuracy:** How often do we predict correctly?\n",
    "- **Precision and Recall:** Especially for the >50K class (minority class)\n",
    "- **Confusion Matrix:** Where does our model make mistakes?\n",
    "- **Class imbalance:** Do we predict the majority class (â‰¤50K) too often?\n",
    "\n",
    "### Important Questions:\n",
    "- Is the model better at predicting â‰¤50K or >50K incomes?\n",
    "- What's the trade-off between precision and recall?\n",
    "- How does performance compare to always predicting the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b50119-24a6-4e82-9e38-930080652dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train final k-NN model and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf892a7-7587-4802-bbce-de9beb4b94a8",
   "metadata": {},
   "source": [
    "## Step 6: Confusion Matrix Analysis\n",
    "\n",
    "Create and analyse the confusion matrix to understand your model's performance in detail.\n",
    "\n",
    "### What the Confusion Matrix Shows:\n",
    "- **True Negatives (TN):** Correctly predicted â‰¤50K\n",
    "- **False Positives (FP):** Incorrectly predicted >50K (Type I error)\n",
    "- **False Negatives (FN):** Incorrectly predicted â‰¤50K (Type II error)  \n",
    "- **True Positives (TP):** Correctly predicted >50K\n",
    "\n",
    "### Key Analysis Questions:\n",
    "- Does the model have a bias toward predicting one class?\n",
    "- Which type of error is more common?\n",
    "- How does this compare to the Glass Classification results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a47dbb-2cf8-4eeb-bd49-5941418ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and visualise the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3blz0l948",
   "metadata": {},
   "source": [
    "## Reflection and Next Steps\n",
    "\n",
    "### Questions to Consider:\n",
    "1. **How did your k-NN model perform?** Compare your accuracy to the baseline.\n",
    "2. **What was the biggest challenge?** Handling categorical variables? Class imbalance?\n",
    "3. **Which features seem most important?** Based on your visualisations, what drives high income?\n",
    "4. **How did this differ from Glass Classification?** What made this problem harder/easier?\n",
    "\n",
    "### Key Learnings:\n",
    "- **Real-world data is messy** - missing values and categorical variables are common\n",
    "- **Feature preprocessing is crucial** - especially encoding and scaling\n",
    "- **Class imbalance matters** - accuracy alone might be misleading\n",
    "- **Domain knowledge helps** - understanding what features mean aids interpretation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
